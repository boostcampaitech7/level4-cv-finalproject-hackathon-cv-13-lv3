{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import random\n",
    "\n",
    "class CLAPDataset(Dataset):\n",
    "    def __init__(self, json_path, processor, sample_rate=48000, max_annotations=None):\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "        self.dirname = os.path.dirname(json_path)\n",
    "\n",
    "        task_filter=\"audiocaption\"\n",
    "\n",
    "        # JSON 파일 로드\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        # task가 audiocaption인 annotation만 필터링\n",
    "        filtered_annotations = [\n",
    "            entry for entry in data[\"annotation\"] if entry[\"task\"] == task_filter\n",
    "        ]\n",
    "        self.annotations = filtered_annotations\n",
    "\n",
    "        if max_annotations is not None:\n",
    "            self.annotations = random.sample(filtered_annotations, min(len(filtered_annotations), max_annotations))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.annotations[idx]\n",
    "        audio_path = os.path.join(self.dirname, entry[\"path\"])\n",
    "        text = entry[\"text\"]\n",
    "\n",
    "        # # 오디오 로드\n",
    "        # audio, sr = sf.read(audio_path)\n",
    "        # # 모노 채널로 변환 (스테레오인 경우)\n",
    "        # if waveform.shape[0] > 1:  # Check if stereo\n",
    "        #     audio = torch.mean(waveform, dim=0, keepdim=True) # 평균 내기\n",
    "            \n",
    "        # if sr != self.sample_rate:\n",
    "        #     waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # librosa를 사용하여 오디오 로드 및 모노 채널 변환, resample\n",
    "        waveform, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True) # 한줄로 처리\n",
    "\n",
    "        length = len(waveform)  # (채널, 길이) 중 길이\n",
    "        \n",
    "        waveform = torch.from_numpy(waveform).float() # tensor로 변환 및 차원 추가\n",
    "\n",
    "        # 오디오 길이 저장 (패딩 최소화를 위해 사용)\n",
    "\n",
    "        return {\"path\": audio_path, \"waveform\": waveform, \"text\": text, \"length\": length}\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # 배치 내 최대 길이 찾기\n",
    "    max_length = max(map(lambda x: x[\"length\"], batch))\n",
    "\n",
    "    paths = []\n",
    "    # 패딩 적용\n",
    "    padded_waveforms = []\n",
    "    texts = []\n",
    "    lengths = []\n",
    "    \n",
    "    for item in batch:\n",
    "        path = item[\"path\"]\n",
    "        waveform = item[\"waveform\"]\n",
    "        text = item[\"text\"]\n",
    "        length = item[\"length\"]\n",
    "\n",
    "        # 오른쪽 패딩 적용\n",
    "        pad_amount = max_length - length\n",
    "        padded_waveform = F.pad(waveform, (0, pad_amount))  # (채널, 길이)\n",
    "        \n",
    "        paths.append(path)\n",
    "        padded_waveforms.append(padded_waveform)\n",
    "        texts.append(text)\n",
    "        lengths.append(length)\n",
    "\n",
    "    # 텐서 변환\n",
    "    padded_waveforms = torch.stack(padded_waveforms)\n",
    "    return {\"paths\": paths, \"waveforms\": padded_waveforms, \"texts\": texts, \"lengths\": lengths}\n",
    "\n",
    "class LengthBatchSampler(BatchSampler):\n",
    "    \"\"\" 비슷한 길이끼리 배치하여 패딩을 최소화하는 샘플러 \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        sorted_indices = sorted(range(len(dataset)), key=lambda i: dataset[i][\"length\"])\n",
    "        batches = [sorted_indices[i:i+batch_size] for i in range(0, len(sorted_indices), batch_size)]\n",
    "        super().__init__(batches, batch_size, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def recreate_output_folder(output_path):\n",
    "    \"\"\"\n",
    "    output 폴더를 삭제하고 새로 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): output 폴더 경로\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # output 폴더가 존재하면 삭제\n",
    "        if os.path.exists(output_path):\n",
    "            shutil.rmtree(output_path)  # 하위 파일 및 폴더 모두 삭제\n",
    "            print(f\"기존 output 폴더({output_path})를 삭제했습니다.\")\n",
    "\n",
    "        # output 폴더 생성\n",
    "        os.makedirs(output_path)\n",
    "        print(f\"새로운 output 폴더({output_path})를 생성했습니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "\n",
    "# 사용 예시\n",
    "output_path = \"output\"  # output 폴더 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process(device, model, processor, dataloader, similarity_threshold =0.5):\n",
    "    results=[]\n",
    "    # 배치 처리\n",
    "    with torch.no_grad(), tqdm(total=len(dataloader), desc=\"Processing Batches\") as pbar:  # tqdm 적용\n",
    "        for batch in dataloader:\n",
    "            paths = batch[\"paths\"]\n",
    "            waveforms = batch[\"waveforms\"].to(device)  # (batch, 채널, 길이)\n",
    "            texts = batch[\"texts\"]\n",
    "            lengths = batch.get(\"lengths\") # lengths 정보 추가 (패딩 고려)\n",
    "\n",
    "            # 오디오 전처리 (💡 배치 단위 변환)\n",
    "            with torch.cuda.amp.autocast():  # ✅ AMP (자동 혼합 정밀도) 적용하여 연산 속도 개선\n",
    "                audio_inputs = processor(audios=waveforms.cpu().numpy(), return_tensors=\"pt\", sampling_rate=48000)\n",
    "                audio_inputs = {k: v.to(device, non_blocking=True) for k, v in audio_inputs.items()}\n",
    "                audio_embeds = model.get_audio_features(**audio_inputs)\n",
    "                audio_embeds = F.normalize(audio_embeds, p=2, dim=-1)\n",
    "\n",
    "                # 텍스트 전처리 (💡 배치 단위 변환)\n",
    "                text_inputs = processor(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                text_inputs = {k: v.to(device, non_blocking=True) for k, v in text_inputs.items()}\n",
    "                text_embeds = model.get_text_features(**text_inputs)\n",
    "                text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "                # 코사인 유사도 계산 (💡 GPU에서 바로 계산)\n",
    "                similarities = F.cosine_similarity(audio_embeds, text_embeds).cpu().numpy()\n",
    "\n",
    "            # 배치 내의 각 샘플에 대해 결과 저장\n",
    "            for i in range(len(texts)):\n",
    "                \n",
    "                if similarities[i] <= similarity_threshold:\n",
    "                    results.append({\n",
    "                        \"similarity\": similarities[i],\n",
    "                        \"path\": paths[i],\n",
    "                        \"text\": texts[i],\n",
    "                        \"original_length\": lengths[i] if lengths is not None else waveforms.shape[2] # 패딩 감안한 원래 길이\n",
    "                    })\n",
    "                    # 심볼릭 링크 생성 시 유사도 값 추가\n",
    "                    basename = os.path.basename(paths[i])\n",
    "                    similarity_str = f\"{similarities[i]:.4f}\"  # 유사도 값을 문자열로 변환 (소수점 4자리까지)\n",
    "                    text_str = f\"{texts[i]}\"\n",
    "                    target_link = os.path.join(\"output\", f\"{similarity_str}_{text_str}_{basename}\") # 파일 이름에 유사도 추가\n",
    "\n",
    "                    if not os.path.exists(target_link):\n",
    "                        os.symlink(os.path.abspath(paths[i]), target_link)\n",
    "            pbar.update(1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/salmonn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clap Loaded\n",
      "ClapProcessor Loaded\n",
      "DataLoader Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "# 모델 및 프로세서 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device).eval()\n",
    "print(\"Clap Loaded\")\n",
    "processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "print(\"ClapProcessor Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 output 폴더(output)를 삭제했습니다.\n",
      "새로운 output 폴더(output)를 생성했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_829507/3669828763.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # ✅ AMP (자동 혼합 정밀도) 적용하여 연산 속도 개선\n",
      "Processing Batches: 100%|██████████| 125/125 [01:36<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     similarity                                               path  \\\n",
      "126   -0.156114  /data/dataset/WavCaps/AudioSet_SL/YODCU2DpDjis...   \n",
      "385   -0.146746  /data/dataset/WavCaps/AudioSet_SL/YZiTjBwBNqsA...   \n",
      "664   -0.126790  /data/dataset/WavCaps/AudioSet_SL/YWWb0zt8q_7g...   \n",
      "560   -0.118316  /data/dataset/WavCaps/FreeSound/30000-60000/10...   \n",
      "528   -0.116468  /data/dataset/WavCaps/AudioSet_SL/YoAs15HQ3LFY...   \n",
      "\n",
      "                                                  text  original_length  \n",
      "126  People are speaking, birds are chirping, and e...           480000  \n",
      "385  A group of people are laughing, speaking and m...           480000  \n",
      "664  People are laughing, talking, and making sound...           480000  \n",
      "560  Kids are screaming, jumping, playing, and yell...          1629156  \n",
      "528  Music and singing are heard along with laughte...           480000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "recreate_output_folder(output_path)\n",
    "\n",
    "# 데이터셋 및 데이터로더 설정\n",
    "dataset = CLAPDataset(\"/data/dataset/stage1_valid_10.json\", processor, max_annotations=1000)\n",
    "dataloader = DataLoader(dataset, collate_fn=pad_collate_fn, batch_size=8, num_workers=8)\n",
    "\n",
    "print(\"DataLoader Loaded\")\n",
    "df = pd.DataFrame(process(device, model, processor, dataloader, similarity_threshold=0.1)).sort_values(by=\"similarity\")\n",
    "\n",
    "# 유사도 낮은 순으로 정렬\n",
    "print(df.head())\n",
    "\n",
    "# DataFrame 저장 (선택적으로 csv, pickle, 등 다른 형식으로 저장 가능)\n",
    "df.to_csv(\"results.csv\", index=False)  # csv 파일로 저장\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salmonn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
