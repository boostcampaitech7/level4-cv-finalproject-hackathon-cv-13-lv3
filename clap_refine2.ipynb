{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import random\n",
    "\n",
    "class CLAPDataset(Dataset):\n",
    "    def __init__(self, json_path, processor, sample_rate=48000, max_annotations=None):\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "        self.dirname = os.path.dirname(json_path)\n",
    "\n",
    "        task_filter=\"audiocaption\"\n",
    "\n",
    "        # JSON íŒŒì¼ ë¡œë“œ\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        # taskê°€ audiocaptionì¸ annotationë§Œ í•„í„°ë§\n",
    "        filtered_annotations = [\n",
    "            entry for entry in data[\"annotation\"] if entry[\"task\"] == task_filter\n",
    "        ]\n",
    "        self.annotations = filtered_annotations\n",
    "\n",
    "        if max_annotations is not None:\n",
    "            self.annotations = random.sample(filtered_annotations, min(len(filtered_annotations), max_annotations))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.annotations[idx]\n",
    "        audio_path = os.path.join(self.dirname, entry[\"path\"])\n",
    "        text = entry[\"text\"]\n",
    "\n",
    "        # # ì˜¤ë””ì˜¤ ë¡œë“œ\n",
    "        # audio, sr = sf.read(audio_path)\n",
    "        # # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜ (ìŠ¤í…Œë ˆì˜¤ì¸ ê²½ìš°)\n",
    "        # if waveform.shape[0] > 1:  # Check if stereo\n",
    "        #     audio = torch.mean(waveform, dim=0, keepdim=True) # í‰ê·  ë‚´ê¸°\n",
    "            \n",
    "        # if sr != self.sample_rate:\n",
    "        #     waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # librosaë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë¡œë“œ ë° ëª¨ë…¸ ì±„ë„ ë³€í™˜, resample\n",
    "        waveform, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True) # í•œì¤„ë¡œ ì²˜ë¦¬\n",
    "\n",
    "        length = len(waveform)  # (ì±„ë„, ê¸¸ì´) ì¤‘ ê¸¸ì´\n",
    "        \n",
    "        waveform = torch.from_numpy(waveform).float() # tensorë¡œ ë³€í™˜ ë° ì°¨ì› ì¶”ê°€\n",
    "\n",
    "        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì €ì¥ (íŒ¨ë”© ìµœì†Œí™”ë¥¼ ìœ„í•´ ì‚¬ìš©)\n",
    "\n",
    "        return {\"path\": audio_path, \"waveform\": waveform, \"text\": text, \"length\": length}\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # ë°°ì¹˜ ë‚´ ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°\n",
    "    max_length = max(map(lambda x: x[\"length\"], batch))\n",
    "\n",
    "    paths = []\n",
    "    # íŒ¨ë”© ì ìš©\n",
    "    padded_waveforms = []\n",
    "    texts = []\n",
    "    lengths = []\n",
    "    \n",
    "    for item in batch:\n",
    "        path = item[\"path\"]\n",
    "        waveform = item[\"waveform\"]\n",
    "        text = item[\"text\"]\n",
    "        length = item[\"length\"]\n",
    "\n",
    "        # ì˜¤ë¥¸ìª½ íŒ¨ë”© ì ìš©\n",
    "        pad_amount = max_length - length\n",
    "        padded_waveform = F.pad(waveform, (0, pad_amount))  # (ì±„ë„, ê¸¸ì´)\n",
    "        \n",
    "        paths.append(path)\n",
    "        padded_waveforms.append(padded_waveform)\n",
    "        texts.append(text)\n",
    "        lengths.append(length)\n",
    "\n",
    "    # í…ì„œ ë³€í™˜\n",
    "    padded_waveforms = torch.stack(padded_waveforms)\n",
    "    return {\"paths\": paths, \"waveforms\": padded_waveforms, \"texts\": texts, \"lengths\": lengths}\n",
    "\n",
    "class LengthBatchSampler(BatchSampler):\n",
    "    \"\"\" ë¹„ìŠ·í•œ ê¸¸ì´ë¼ë¦¬ ë°°ì¹˜í•˜ì—¬ íŒ¨ë”©ì„ ìµœì†Œí™”í•˜ëŠ” ìƒ˜í”ŒëŸ¬ \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        sorted_indices = sorted(range(len(dataset)), key=lambda i: dataset[i][\"length\"])\n",
    "        batches = [sorted_indices[i:i+batch_size] for i in range(0, len(sorted_indices), batch_size)]\n",
    "        super().__init__(batches, batch_size, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def recreate_output_folder(output_path):\n",
    "    \"\"\"\n",
    "    output í´ë”ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): output í´ë” ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # output í´ë”ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ\n",
    "        if os.path.exists(output_path):\n",
    "            shutil.rmtree(output_path)  # í•˜ìœ„ íŒŒì¼ ë° í´ë” ëª¨ë‘ ì‚­ì œ\n",
    "            print(f\"ê¸°ì¡´ output í´ë”({output_path})ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # output í´ë” ìƒì„±\n",
    "        os.makedirs(output_path)\n",
    "        print(f\"ìƒˆë¡œìš´ output í´ë”({output_path})ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "output_path = \"output\"  # output í´ë” ê²½ë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process(device, model, processor, dataloader, similarity_threshold =0.5):\n",
    "    results=[]\n",
    "    # ë°°ì¹˜ ì²˜ë¦¬\n",
    "    with torch.no_grad(), tqdm(total=len(dataloader), desc=\"Processing Batches\") as pbar:  # tqdm ì ìš©\n",
    "        for batch in dataloader:\n",
    "            paths = batch[\"paths\"]\n",
    "            waveforms = batch[\"waveforms\"].to(device)  # (batch, ì±„ë„, ê¸¸ì´)\n",
    "            texts = batch[\"texts\"]\n",
    "            lengths = batch.get(\"lengths\") # lengths ì •ë³´ ì¶”ê°€ (íŒ¨ë”© ê³ ë ¤)\n",
    "\n",
    "            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ğŸ’¡ ë°°ì¹˜ ë‹¨ìœ„ ë³€í™˜)\n",
    "            with torch.cuda.amp.autocast():  # âœ… AMP (ìë™ í˜¼í•© ì •ë°€ë„) ì ìš©í•˜ì—¬ ì—°ì‚° ì†ë„ ê°œì„ \n",
    "                audio_inputs = processor(audios=waveforms.cpu().numpy(), return_tensors=\"pt\", sampling_rate=48000)\n",
    "                audio_inputs = {k: v.to(device, non_blocking=True) for k, v in audio_inputs.items()}\n",
    "                audio_embeds = model.get_audio_features(**audio_inputs)\n",
    "                audio_embeds = F.normalize(audio_embeds, p=2, dim=-1)\n",
    "\n",
    "                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ğŸ’¡ ë°°ì¹˜ ë‹¨ìœ„ ë³€í™˜)\n",
    "                text_inputs = processor(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                text_inputs = {k: v.to(device, non_blocking=True) for k, v in text_inputs.items()}\n",
    "                text_embeds = model.get_text_features(**text_inputs)\n",
    "                text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ğŸ’¡ GPUì—ì„œ ë°”ë¡œ ê³„ì‚°)\n",
    "                similarities = F.cosine_similarity(audio_embeds, text_embeds).cpu().numpy()\n",
    "\n",
    "            # ë°°ì¹˜ ë‚´ì˜ ê° ìƒ˜í”Œì— ëŒ€í•´ ê²°ê³¼ ì €ì¥\n",
    "            for i in range(len(texts)):\n",
    "                \n",
    "                if similarities[i] <= similarity_threshold:\n",
    "                    results.append({\n",
    "                        \"similarity\": similarities[i],\n",
    "                        \"path\": paths[i],\n",
    "                        \"text\": texts[i],\n",
    "                        \"original_length\": lengths[i] if lengths is not None else waveforms.shape[2] # íŒ¨ë”© ê°ì•ˆí•œ ì›ë˜ ê¸¸ì´\n",
    "                    })\n",
    "                    # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹œ ìœ ì‚¬ë„ ê°’ ì¶”ê°€\n",
    "                    basename = os.path.basename(paths[i])\n",
    "                    similarity_str = f\"{similarities[i]:.4f}\"  # ìœ ì‚¬ë„ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€)\n",
    "                    text_str = f\"{texts[i]}\"\n",
    "                    target_link = os.path.join(\"output\", f\"{similarity_str}_{text_str}_{basename}\") # íŒŒì¼ ì´ë¦„ì— ìœ ì‚¬ë„ ì¶”ê°€\n",
    "\n",
    "                    if not os.path.exists(target_link):\n",
    "                        os.symlink(os.path.abspath(paths[i]), target_link)\n",
    "            pbar.update(1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/salmonn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clap Loaded\n",
      "ClapProcessor Loaded\n",
      "DataLoader Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "# ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device).eval()\n",
    "print(\"Clap Loaded\")\n",
    "processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "print(\"ClapProcessor Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ output í´ë”(output)ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\n",
      "ìƒˆë¡œìš´ output í´ë”(output)ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_829507/3669828763.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # âœ… AMP (ìë™ í˜¼í•© ì •ë°€ë„) ì ìš©í•˜ì—¬ ì—°ì‚° ì†ë„ ê°œì„ \n",
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:36<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     similarity                                               path  \\\n",
      "126   -0.156114  /data/dataset/WavCaps/AudioSet_SL/YODCU2DpDjis...   \n",
      "385   -0.146746  /data/dataset/WavCaps/AudioSet_SL/YZiTjBwBNqsA...   \n",
      "664   -0.126790  /data/dataset/WavCaps/AudioSet_SL/YWWb0zt8q_7g...   \n",
      "560   -0.118316  /data/dataset/WavCaps/FreeSound/30000-60000/10...   \n",
      "528   -0.116468  /data/dataset/WavCaps/AudioSet_SL/YoAs15HQ3LFY...   \n",
      "\n",
      "                                                  text  original_length  \n",
      "126  People are speaking, birds are chirping, and e...           480000  \n",
      "385  A group of people are laughing, speaking and m...           480000  \n",
      "664  People are laughing, talking, and making sound...           480000  \n",
      "560  Kids are screaming, jumping, playing, and yell...          1629156  \n",
      "528  Music and singing are heard along with laughte...           480000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "recreate_output_folder(output_path)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •\n",
    "dataset = CLAPDataset(\"/data/dataset/stage1_valid_10.json\", processor, max_annotations=1000)\n",
    "dataloader = DataLoader(dataset, collate_fn=pad_collate_fn, batch_size=8, num_workers=8)\n",
    "\n",
    "print(\"DataLoader Loaded\")\n",
    "df = pd.DataFrame(process(device, model, processor, dataloader, similarity_threshold=0.1)).sort_values(by=\"similarity\")\n",
    "\n",
    "# ìœ ì‚¬ë„ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "print(df.head())\n",
    "\n",
    "# DataFrame ì €ì¥ (ì„ íƒì ìœ¼ë¡œ csv, pickle, ë“± ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ ê°€ëŠ¥)\n",
    "df.to_csv(\"results.csv\", index=False)  # csv íŒŒì¼ë¡œ ì €ì¥\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salmonn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
